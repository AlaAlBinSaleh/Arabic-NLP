{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim\n",
    "import spacy \n",
    "import nltk \n",
    "from nltk import ngrams\n",
    "import re\n",
    "#from utilities import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the cleaned and preprocessed dataset \n",
    "ArNews_df_update = pd.read_csv(\"/Users/AlaAlBinSaleh/Desktop/Desktop/Capstone/ArNews_df_Cleaned.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1)Word Embding using AraVec \n",
    "model = gensim.models.Word2Vec.load(\"full_grams_cbow_100_wiki/full_grams_cbow_100_wiki.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've 662109 vocabularies\n"
     ]
    }
   ],
   "source": [
    "#test the model vocbulary \n",
    "print(\"We've\",len(model.wv.index_to_key),\"vocabularies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: spacyModel: File exists\n"
     ]
    }
   ],
   "source": [
    "%mkdir spacyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip ./spacyModel/aravec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Creating blank nlp object for language 'ar'\u001b[0m\n",
      "[2021-11-12 23:16:03,453] [INFO] Reading vectors from spacyModel/aravec.txt.gz\n",
      "662109it [01:21, 8129.57it/s]\n",
      "[2021-11-12 23:17:24,926] [INFO] Loaded vectors from spacyModel/aravec.txt.gz\n",
      "\u001b[38;5;2m✔ Successfully converted 662109 vectors\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
      "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
      "/Users/AlaAlBinSaleh/Desktop/Desktop/Capstone/spacy.aravec.model\n"
     ]
    }
   ],
   "source": [
    "#!python3 -m spacy init ar spacy.aravec.model --vectors-loc ./spacyModel/aravec.txt.gz\n",
    "!python3 -m spacy init vectors ar ./spacyModel/aravec.txt.gz ./spacy.aravec.model\n",
    "#spacy.aravec.model --vectors-loc ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./spacy.aravec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing Class\n",
    "class Preprocessor:\n",
    "    def __init__(self, tokenizer, **cfg):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text):\n",
    "        preprocessed = clean_str(text)\n",
    "        return self.tokenizer(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Preprocessor at 0x7fd093d20e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer = Preprocessor(nlp.tokenizer)\n",
    "nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3541306 , -0.47691634, -2.037478  ,  1.7818414 ,  3.2348447 ,\n",
       "        2.9923422 , -1.6732128 ,  2.8066268 , -0.13688694,  1.7843444 ,\n",
       "        0.9327977 , -0.9078639 , -1.3284395 , -0.6065882 ,  3.0176415 ,\n",
       "       -4.276657  ,  1.2542839 ,  0.89644   ,  4.40036   , -3.3811958 ,\n",
       "        3.0062063 , -1.9969931 ,  2.0136075 ,  0.8734018 ,  5.452243  ,\n",
       "        1.7340939 ,  1.3925358 , -1.7027333 , -0.56593776, -3.1578138 ,\n",
       "       -2.6459064 , -1.2861159 ,  2.8269248 , -2.9818544 ,  3.3686252 ,\n",
       "       -2.1550925 ,  0.562423  , -1.5678904 ,  2.0604756 ,  1.0104055 ,\n",
       "        0.949106  ,  3.5613863 ,  0.21240458,  0.69437015,  0.9959716 ,\n",
       "       -1.5333009 ,  1.1857115 , -0.6239423 ,  0.79202735, -3.4440598 ,\n",
       "        2.4681935 , -0.19734618, -1.057888  ,  2.6490996 , -3.813241  ,\n",
       "       -6.690389  , -1.9947684 , -0.01682599,  3.6795888 , -1.0036598 ,\n",
       "       -0.08111735, -4.285988  ,  0.13309926, -3.1279614 , -4.398629  ,\n",
       "        2.2460012 ,  6.380341  , -1.0859216 ,  1.4137076 ,  1.1164379 ,\n",
       "        3.1134524 ,  4.009286  , -2.5007036 , -0.73626846, -0.01842057,\n",
       "       -0.7893826 ,  4.446756  ,  4.1396494 , -0.78314596, -0.04267331,\n",
       "       -2.4427257 ,  2.1267383 ,  3.4071918 ,  3.6840377 ,  0.863308  ,\n",
       "       -2.5711865 , -3.0974073 , -4.922259  , -2.2190256 , -1.642871  ,\n",
       "        1.4137864 , -1.3749678 ,  0.75264925, -3.1450534 , -1.1147295 ,\n",
       "        0.92111593,  1.7925801 ,  3.5235875 , -0.72595114,  2.1060352 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the word embedding \n",
    "nlp(\"بيانات\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('البيانات', 0.8408971428871155),\n",
       " ('معلومات', 0.7972483038902283),\n",
       " ('قاعده_بيانات', 0.7843179702758789),\n",
       " ('حسابات', 0.7556501626968384),\n",
       " ('المعلومات', 0.7542430758476257),\n",
       " ('قاعده_البيانات', 0.7380597591400146),\n",
       " ('للبيانات', 0.7246543169021606),\n",
       " ('رسائل_البريد_الالكتروني', 0.7245200276374817),\n",
       " ('قواعد_بيانات', 0.7238749861717224),\n",
       " ('وبيانات', 0.7214061617851257)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"بيانات\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3541306 , -0.47691634, -2.037478  ,  1.7818414 ,  3.2348447 ,\n",
       "        2.9923422 , -1.6732128 ,  2.8066268 , -0.13688694,  1.7843444 ,\n",
       "        0.9327977 , -0.9078639 , -1.3284395 , -0.6065882 ,  3.0176415 ,\n",
       "       -4.276657  ,  1.2542839 ,  0.89644   ,  4.40036   , -3.3811958 ,\n",
       "        3.0062063 , -1.9969931 ,  2.0136075 ,  0.8734018 ,  5.452243  ,\n",
       "        1.7340939 ,  1.3925358 , -1.7027333 , -0.56593776, -3.1578138 ,\n",
       "       -2.6459064 , -1.2861159 ,  2.8269248 , -2.9818544 ,  3.3686252 ,\n",
       "       -2.1550925 ,  0.562423  , -1.5678904 ,  2.0604756 ,  1.0104055 ,\n",
       "        0.949106  ,  3.5613863 ,  0.21240458,  0.69437015,  0.9959716 ,\n",
       "       -1.5333009 ,  1.1857115 , -0.6239423 ,  0.79202735, -3.4440598 ,\n",
       "        2.4681935 , -0.19734618, -1.057888  ,  2.6490996 , -3.813241  ,\n",
       "       -6.690389  , -1.9947684 , -0.01682599,  3.6795888 , -1.0036598 ,\n",
       "       -0.08111735, -4.285988  ,  0.13309926, -3.1279614 , -4.398629  ,\n",
       "        2.2460012 ,  6.380341  , -1.0859216 ,  1.4137076 ,  1.1164379 ,\n",
       "        3.1134524 ,  4.009286  , -2.5007036 , -0.73626846, -0.01842057,\n",
       "       -0.7893826 ,  4.446756  ,  4.1396494 , -0.78314596, -0.04267331,\n",
       "       -2.4427257 ,  2.1267383 ,  3.4071918 ,  3.6840377 ,  0.863308  ,\n",
       "       -2.5711865 , -3.0974073 , -4.922259  , -2.2190256 , -1.642871  ,\n",
       "        1.4137864 , -1.3749678 ,  0.75264925, -3.1450534 , -1.1147295 ,\n",
       "        0.92111593,  1.7925801 ,  3.5235875 , -0.72595114,  2.1060352 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"بيانات\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ArNews_df_update[\"Detokenize\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word_embedding(text):\n",
    "    global word_embeddings\n",
    "    word_embeddings = []\n",
    "    for line in ArNews_df_update[\"content\"]:\n",
    "        avgword2vec = None\n",
    "        counter = 0\n",
    "        #split into sentences \n",
    "        for word in line.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                counter += 1\n",
    "                # words embedding of sentences \n",
    "                if avgword2vec is None:\n",
    "                    avgword2vec = model.wv[word]\n",
    "                else:\n",
    "                    avgword2vec = avgword2vec + model.wv[word]\n",
    "        if avgword2vec is not None:\n",
    "            avgword2vec = avgword2vec / counter # normalize sum\n",
    "            word_embeddings.append(avgword2vec)\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec =  Word_embedding(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = re.sub(r'\\s*[A-Za-z]+\\b', '' , text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "word=[]\n",
    "\n",
    "for i in range(len(ArNews_df_update['Detokenize'])):\n",
    "        word =ArNews_df_update['Detokenize'].iloc[i]\n",
    "        corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in str(document).lower().split()]for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(45199 unique tokens: ['أضف', 'ابن', 'اخر', 'اذ', 'اضة']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "#print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.018*\"دول\" + 0.012*\"امر\" + 0.011*\"اير\" + 0.010*\"تفق\" + 0.010*\"تحد\" + 0.010*\"حكم\" + 0.009*\"عمل\" + 0.009*\"سيس\" + 0.008*\"رئس\" + 0.008*\"قرر\"\n",
      "Topic: 1 \n",
      "Words: 0.023*\"شرك\" + 0.016*\"نسب\" + 0.015*\"سوق\" + 0.014*\"سعر\" + 0.013*\"رفع\" + 0.012*\"سهم\" + 0.012*\"عمل\" + 0.011*\"ريل\" + 0.010*\"قطع\" + 0.010*\"ائة\"\n",
      "Topic: 2 \n",
      "Words: 0.026*\"جمع\" + 0.020*\"وزر\" + 0.017*\"عمل\" + 0.015*\"طلب\" + 0.014*\"سعد\" + 0.010*\"درس\" + 0.010*\"قدم\" + 0.009*\"خدم\" + 0.008*\"خلل\" + 0.008*\"جلس\"\n",
      "Topic: 3 \n",
      "Words: 0.014*\"قتل\" + 0.012*\"نظم\" + 0.010*\"يمن\" + 0.010*\"عمل\" + 0.010*\"سور\" + 0.010*\"حوث\" + 0.009*\"قوت\" + 0.009*\"نطق\" + 0.009*\"عسكر\" + 0.008*\"دعش\"\n",
      "Topic: 4 \n",
      "Words: 0.033*\"سعد\" + 0.022*\"امر\" + 0.021*\"حمد\" + 0.019*\"رئس\" + 0.018*\"ملك\" + 0.017*\"سلم\" + 0.016*\"عبدالعزيز\" + 0.015*\"شرف\" + 0.014*\"الل\" + 0.013*\"سمو\"\n",
      "Topic: 5 \n",
      "Words: 0.019*\"عمل\" + 0.018*\"نطق\" + 0.011*\"طرق\" + 0.010*\"خدم\" + 0.009*\"حفظ\" + 0.008*\"بلغ\" + 0.008*\"نفذ\" + 0.007*\"خلل\" + 0.007*\"جمع\" + 0.007*\"دير\"\n",
      "Topic: 6 \n",
      "Words: 0.027*\"لعب\" + 0.026*\"فرق\" + 0.018*\"ندي\" + 0.011*\"درب\" + 0.011*\"قدم\" + 0.010*\"تحد\" + 0.009*\"وسم\" + 0.008*\"دور\" + 0.008*\"سعد\" + 0.008*\"بطل\"\n",
      "Topic: 7 \n",
      "Words: 0.014*\"نطق\" + 0.014*\"عرض\" + 0.013*\"جمع\" + 0.012*\"سيح\" + 0.012*\"فعل\" + 0.010*\"هرج\" + 0.009*\"قدم\" + 0.008*\"عمل\" + 0.008*\"ثقف\" + 0.008*\"بلد\"\n",
      "Topic: 8 \n",
      "Words: 0.018*\"سلم\" + 0.016*\"امن\" + 0.015*\"وطن\" + 0.015*\"رهب\" + 0.013*\"جمع\" + 0.012*\"الل\" + 0.009*\"حكم\" + 0.009*\"سجد\" + 0.009*\"جرم\" + 0.008*\"عمل\"\n",
      "Topic: 9 \n",
      "Words: 0.009*\"كثر\" + 0.009*\"جمع\" + 0.007*\"حدث\" + 0.007*\"كتب\" + 0.007*\"عمل\" + 0.006*\"عرف\" + 0.006*\"شكل\" + 0.005*\"شعر\" + 0.005*\"فكر\" + 0.005*\"كبر\"\n"
     ]
    }
   ],
   "source": [
    "#print(lda.print_topics(num_topics=10, num_words=15), \"\\n\")\n",
    "for idx, topic in lda.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
