{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim\n",
    "import spacy \n",
    "import nltk \n",
    "from nltk import ngrams\n",
    "#from utilities import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the cleaned and preprocessed dataset \n",
    "ArNews_df_update = pd.read_csv(\"/Users/AlaAlBinSaleh/Desktop/Desktop/Capstone/ArNews_df_Cleaned.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1)Word Embding using AraVec \n",
    "model = gensim.models.Word2Vec.load(\"full_grams_cbow_100_wiki/full_grams_cbow_100_wiki.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model2 = gensim.models.Word2Vec.load(\"/Users/AlaAlBinSaleh/Desktop/Desktop/Capstone/full_grams_cbow_100_wiki/full_grams_cbow_100_wiki.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've 662109 vocabularies\n"
     ]
    }
   ],
   "source": [
    "#test the model vocbulary \n",
    "print(\"We've\",len(model.wv.index_to_key),\"vocabularies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: spacyModel: File exists\n"
     ]
    }
   ],
   "source": [
    "%mkdir spacyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip ./spacyModel/aravec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Creating blank nlp object for language 'ar'\u001b[0m\n",
      "[2021-11-12 23:16:03,453] [INFO] Reading vectors from spacyModel/aravec.txt.gz\n",
      "662109it [01:21, 8129.57it/s]\n",
      "[2021-11-12 23:17:24,926] [INFO] Loaded vectors from spacyModel/aravec.txt.gz\n",
      "\u001b[38;5;2m✔ Successfully converted 662109 vectors\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
      "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
      "/Users/AlaAlBinSaleh/Desktop/Desktop/Capstone/spacy.aravec.model\n"
     ]
    }
   ],
   "source": [
    "#!python3 -m spacy init ar spacy.aravec.model --vectors-loc ./spacyModel/aravec.txt.gz\n",
    "!python3 -m spacy init vectors ar ./spacyModel/aravec.txt.gz ./spacy.aravec.model\n",
    "#spacy.aravec.model --vectors-loc ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./spacy.aravec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3541306 , -0.47691634, -2.037478  ,  1.7818414 ,  3.2348447 ,\n",
       "        2.9923422 , -1.6732128 ,  2.8066268 , -0.13688694,  1.7843444 ,\n",
       "        0.9327977 , -0.9078639 , -1.3284395 , -0.6065882 ,  3.0176415 ,\n",
       "       -4.276657  ,  1.2542839 ,  0.89644   ,  4.40036   , -3.3811958 ,\n",
       "        3.0062063 , -1.9969931 ,  2.0136075 ,  0.8734018 ,  5.452243  ,\n",
       "        1.7340939 ,  1.3925358 , -1.7027333 , -0.56593776, -3.1578138 ,\n",
       "       -2.6459064 , -1.2861159 ,  2.8269248 , -2.9818544 ,  3.3686252 ,\n",
       "       -2.1550925 ,  0.562423  , -1.5678904 ,  2.0604756 ,  1.0104055 ,\n",
       "        0.949106  ,  3.5613863 ,  0.21240458,  0.69437015,  0.9959716 ,\n",
       "       -1.5333009 ,  1.1857115 , -0.6239423 ,  0.79202735, -3.4440598 ,\n",
       "        2.4681935 , -0.19734618, -1.057888  ,  2.6490996 , -3.813241  ,\n",
       "       -6.690389  , -1.9947684 , -0.01682599,  3.6795888 , -1.0036598 ,\n",
       "       -0.08111735, -4.285988  ,  0.13309926, -3.1279614 , -4.398629  ,\n",
       "        2.2460012 ,  6.380341  , -1.0859216 ,  1.4137076 ,  1.1164379 ,\n",
       "        3.1134524 ,  4.009286  , -2.5007036 , -0.73626846, -0.01842057,\n",
       "       -0.7893826 ,  4.446756  ,  4.1396494 , -0.78314596, -0.04267331,\n",
       "       -2.4427257 ,  2.1267383 ,  3.4071918 ,  3.6840377 ,  0.863308  ,\n",
       "       -2.5711865 , -3.0974073 , -4.922259  , -2.2190256 , -1.642871  ,\n",
       "        1.4137864 , -1.3749678 ,  0.75264925, -3.1450534 , -1.1147295 ,\n",
       "        0.92111593,  1.7925801 ,  3.5235875 , -0.72595114,  2.1060352 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the vactorization \n",
    "nlp(\"بيانات\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, tokenizer, **cfg):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text):\n",
    "        preprocessed = clean_str(text)\n",
    "        return self.tokenizer(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "#computing words frequancy using tf-idf\n",
    "count_vec=TfidfVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b\")\n",
    "arr = count_vec.fit_transform(ArNews_df_update[\"Detokenize\"]).toarray()\n",
    "print(arr)\n",
    "for key,value in count_vec.vocabulary_.items():\n",
    "    print(key,value)\n",
    "\"\"\"\n",
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
